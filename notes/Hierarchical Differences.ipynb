{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in Hierarchical Annotations\n",
    "\n",
    "Let's investigate the SALAMI annotations first. We will compare:\n",
    "\n",
    "* NCE between upper and lower cases for each track between annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import glob\n",
    "import jams\n",
    "import mir_eval\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Storing the intervals and labels of a segmentation in a named tuple\n",
    "Segmentation = namedtuple('Segmentation', ['inters', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "salami_jam_files = glob.glob(\"/home/uri/Projects/jams-data-salami/datasets/SALAMI/*.jams\")\n",
    "spam_jam_files = glob.glob(\"/home/uri/Projects/msaf-data/SPAM/references/*.jams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(upper, lower):\n",
    "    \"\"\"Computes the metrics for the given segmentations of a single annotator.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    upper: Segmentation\n",
    "        The upper segmentation of the annotator.\n",
    "    lower: Segmentation\n",
    "        The lower segmentation of the annotator.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    res: dict\n",
    "        Dictionary containing the following metrics:\n",
    "        - NCE scores\n",
    "        - number of segments\n",
    "        - number of unique labels\n",
    "        - mean segment length\n",
    "        - number of segments per label\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    \n",
    "    def _compute_layer_metrics(layer, prefix):\n",
    "        \"\"\"Computes single-layer metrics\"\"\"\n",
    "        res[\"%snsegs\" % prefix] = len(layer.labels)\n",
    "        res[\"%snunique_labels\" % prefix] = len(np.unique(layer.labels))\n",
    "        res[\"%smean_seg_dur\" % prefix] = np.mean([inter[1] - inter[0] \n",
    "                                                  for inter in layer.inters])\n",
    "        res[\"%snsegs_per_label\" % prefix] = \\\n",
    "            {label: len(np.where(np.asarray(layer.labels) == label)[0]) \n",
    "             for label in np.unique(layer.labels)}\n",
    "    \n",
    "    # Compute NCE\n",
    "    try:\n",
    "        res[\"S_o\"], res[\"S_u\"], res[\"S_f\"] = \\\n",
    "            mir_eval.segment.nce(upper.inters, upper.labels, \n",
    "                                 lower.inters, lower.labels)\n",
    "    except ValueError:\n",
    "        res[\"S_o\"], res[\"S_u\"], res[\"S_f\"] = (None, None, None)\n",
    "    \n",
    "    # Upper metrics\n",
    "    _compute_layer_metrics(upper, \"upper_\")\n",
    "    \n",
    "    # Lower metrics\n",
    "    _compute_layer_metrics(lower, \"lower_\")\n",
    "    \n",
    "    return res\n",
    "\n",
    "def process_jam(jam_file):\n",
    "    \"\"\"Processes a given jam, obtaining all their annotators and having pairwise comparisons\n",
    "    between upper and lower layers of segmentation.\"\"\"\n",
    "    jam = jams.load(jam_file)\n",
    "    all_res = []\n",
    "    for upper, lower in zip(jam.search(namespace=\"segment_salami_upper\"),\n",
    "                            jam.search(namespace=\"segment_salami_lower\")):\n",
    "        # Make sure we are dealing with the same annotator\n",
    "        assert upper.annotation_metadata.annotator.name == lower.annotation_metadata.annotator.name\n",
    "\n",
    "        # Get actual annotations\n",
    "        upper_seg = Segmentation(*upper.data.to_interval_values())\n",
    "        lower_seg = Segmentation(*lower.data.to_interval_values())\n",
    "\n",
    "        # Compute all metrics\n",
    "        res = compute_metrics(upper_seg, lower_seg)\n",
    "\n",
    "        # Store additional info\n",
    "        res[\"annotator_name\"] = upper.annotation_metadata.annotator.name\n",
    "        res[\"track_dur\"] = jam.file_metadata.duration\n",
    "        res[\"track_name\"] = os.path.basename(jam_file)[:-5]\n",
    "        all_res.append(res)\n",
    "    return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_all_jams(jam_files):\n",
    "    all_data = []\n",
    "    for jam_file in jam_files:\n",
    "        all_data += process_jam(jam_file)\n",
    "    all_df = pd.DataFrame(all_data)\n",
    "    all_df = all_df[[\"track_name\", \"track_dur\", \"annotator_name\",\n",
    "                     u'S_f', u'S_o', u'S_u', \n",
    "                     u'lower_mean_seg_dur',u'lower_nsegs', \n",
    "                     u'lower_nsegs_per_label', u'lower_nunique_labels',\n",
    "                     u'upper_mean_seg_dur', u'upper_nsegs', \n",
    "                     u'upper_nsegs_per_label', u'upper_nunique_labels']]\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute all the salami metrics\n",
    "salami_df = compute_all_jams(salami_jam_files)\n",
    "salami_df.to_csv(\"salami_annotator_metrics.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute all the spam metrics\n",
    "spam_df = compute_all_jams(spam_jam_files)\n",
    "spam_df.to_csv(\"spam_annotator_metrics.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
